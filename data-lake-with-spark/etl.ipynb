{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstrating the ETL process with local input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, to_timestamp, monotonically_increasing_id\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config.get('AWS', 'AWS_ACCESS_KEY_ID')\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config.get('AWS', 'AWS_SECRET_ACCESS_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_data = \"s3a://udacity-dend/\"\n",
    "input_data = \"data/\"\n",
    "output_data = \"output/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process song_data with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n",
      "song_data\n",
      "+------------------+---------------+----------------+-----------------+--------------------+------------------+--------------------+---------+----+\n",
      "|         artist_id|artist_latitude|artist_longitude|  artist_location|         artist_name|           song_id|               title| duration|year|\n",
      "+------------------+---------------+----------------+-----------------+--------------------+------------------+--------------------+---------+----+\n",
      "|ARDR4AC1187FB371A1|           null|            null|                 |Montserrat Caball...|SOBAYLL12A8C138AF9|Sono andati? Fing...|511.16363|   0|\n",
      "|AREBBGV1187FB523D2|           null|            null|      Houston, TX|Mike Jones (Featu...|SOOLYAZ12A6701F4A6|Laws Patrolling (...|173.66159|   0|\n",
      "|ARMAC4T1187FB3FA4C|       40.82624|       -74.47995|Morris Plains, NJ|The Dillinger Esc...|SOBBUGU12A8C13E95D|Setting Fire to S...|207.77751|2004|\n",
      "|ARPBNLO1187FB3D52F|       40.71455|       -74.00712|     New York, NY|            Tiny Tim|SOAOIBZ12AB01815BE|I Hold Your Hand ...| 43.36281|2000|\n",
      "|ARDNS031187B9924F0|       32.67828|       -83.22295|          Georgia|          Tim Wilson|SONYPOM12A8C13B2D7|I Think My Wife I...|186.48771|2005|\n",
      "+------------------+---------------+----------------+-----------------+--------------------+------------------+--------------------+---------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "songs_table\n",
      "+------------------+--------------------+------------------+----+---------+\n",
      "|           song_id|               title|         artist_id|year| duration|\n",
      "+------------------+--------------------+------------------+----+---------+\n",
      "|SOBAYLL12A8C138AF9|Sono andati? Fing...|ARDR4AC1187FB371A1|   0|511.16363|\n",
      "|SOOLYAZ12A6701F4A6|Laws Patrolling (...|AREBBGV1187FB523D2|   0|173.66159|\n",
      "|SOBBUGU12A8C13E95D|Setting Fire to S...|ARMAC4T1187FB3FA4C|2004|207.77751|\n",
      "|SOAOIBZ12AB01815BE|I Hold Your Hand ...|ARPBNLO1187FB3D52F|2000| 43.36281|\n",
      "|SONYPOM12A8C13B2D7|I Think My Wife I...|ARDNS031187B9924F0|2005|186.48771|\n",
      "+------------------+--------------------+------------------+----+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "artists_table\n",
      "+------------------+--------------------+-----------------+---------------+----------------+\n",
      "|         artist_id|         artist_name|  artist_location|artist_latitude|artist_longitude|\n",
      "+------------------+--------------------+-----------------+---------------+----------------+\n",
      "|ARDR4AC1187FB371A1|Montserrat Caball...|                 |           null|            null|\n",
      "|AREBBGV1187FB523D2|Mike Jones (Featu...|      Houston, TX|           null|            null|\n",
      "|ARMAC4T1187FB3FA4C|The Dillinger Esc...|Morris Plains, NJ|       40.82624|       -74.47995|\n",
      "|ARPBNLO1187FB3D52F|            Tiny Tim|     New York, NY|       40.71455|       -74.00712|\n",
      "|ARDNS031187B9924F0|          Tim Wilson|          Georgia|       32.67828|       -83.22295|\n",
      "+------------------+--------------------+-----------------+---------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def process_song_data(spark, input_data, output_data):\n",
    "    # get filepath to song data file\n",
    "    song_data = input_data + \"song_data/*/*/*/*.json\"\n",
    "    song_data1 = input_data + \"song_data/*/*/*/TRABCEI128F424C983.json\"  # to experiment on 1 file\n",
    "    \n",
    "    songdata_schema = StructType([\n",
    "        StructField(\"artist_id\", StringType(), True),\n",
    "        StructField(\"artist_latitude\", DoubleType(), True),\n",
    "        StructField(\"artist_longitude\", DoubleType(), True),\n",
    "        StructField(\"artist_location\", StringType(), True),\n",
    "        StructField(\"artist_name\", StringType(), True),\n",
    "        StructField(\"song_id\", StringType(), True),\n",
    "        StructField(\"title\", StringType(), True),\n",
    "        StructField(\"duration\", DoubleType(), True),\n",
    "        StructField(\"year\", IntegerType(), True),        \n",
    "    ])\n",
    "    \n",
    "    # read song data file\n",
    "    df = spark.read.json(song_data, schema=songdata_schema)\n",
    "    df.printSchema()\n",
    "    print('song_data')\n",
    "    df.show(5)\n",
    "    \n",
    "    # extract columns to create songs table\n",
    "    songs_table = df.select(\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\")\n",
    "    print('songs_table')\n",
    "    songs_table.show(5)\n",
    "    \n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    songs_table_path = os.path.join(output_data, \"songs_table.parquet\")\n",
    "    if not os.path.exists(songs_table_path):\n",
    "        songs_table.write.partitionBy(\"year\", \"artist_id\").parquet(songs_table_path)\n",
    "\n",
    "    # extract columns to create artists table\n",
    "    artists_table = df.select(\"artist_id\", \"artist_name\", \"artist_location\", \n",
    "                              \"artist_latitude\", \"artist_longitude\")\n",
    "    print('artists_table')\n",
    "    artists_table.show(5)\n",
    "    \n",
    "    # write artists table to parquet files\n",
    "    artists_table_path = os.path.join(output_data, \"artists_table.parquet\")\n",
    "    if not os.path.exists(artists_table_path):\n",
    "        artists_table.write.parquet(artists_table_path)\n",
    "    \n",
    "\n",
    "process_song_data(spark, input_data, output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process log_data with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: integer (nullable = true)\n",
      "\n",
      "log_data\n",
      "8056\n",
      "+------+----------+---------+------+-------------+--------+------+-----+--------+------+-----+------------+---------+----+------+-------------+---------+------+\n",
      "|artist|      auth|firstName|gender|itemInSession|lastName|length|level|location|method| page|registration|sessionId|song|status|           ts|userAgent|userId|\n",
      "+------+----------+---------+------+-------------+--------+------+-----+--------+------+-----+------------+---------+----+------+-------------+---------+------+\n",
      "|  null|      null|     null|  null|         null|    null|  null| null|    null|  null| null|        null|     null|null|  null|         null|     null|  null|\n",
      "|  null|      null|     null|  null|         null|    null|  null| null|    null|  null| null|        null|     null|null|  null|         null|     null|  null|\n",
      "|  null|      null|     null|  null|         null|    null|  null| null|    null|  null| null|        null|     null|null|  null|         null|     null|  null|\n",
      "|  null|      null|     null|  null|         null|    null|  null| null|    null|  null| null|        null|     null|null|  null|         null|     null|  null|\n",
      "|  null|      null|     null|  null|         null|    null|  null| null|    null|  null| null|        null|     null|null|  null|         null|     null|  null|\n",
      "|  null|      null|     null|  null|         null|    null|  null| null|    null|  null| null|        null|     null|null|  null|         null|     null|  null|\n",
      "|  null|      null|     null|  null|         null|    null|  null| null|    null|  null| null|        null|     null|null|  null|         null|     null|  null|\n",
      "|  null|Logged Out|     null|  null|            0|    null|  null| paid|    null|   PUT|Login|        null|      602|null|   307|1542260074796|     null|  null|\n",
      "|  null|      null|     null|  null|         null|    null|  null| null|    null|  null| null|        null|     null|null|  null|         null|     null|  null|\n",
      "|  null|      null|     null|  null|         null|    null|  null| null|    null|  null| null|        null|     null|null|  null|         null|     null|  null|\n",
      "+------+----------+---------+------+-------------+--------+------+-----+--------+------+-----+------------+---------+----+------+-------------+---------+------+\n",
      "only showing top 10 rows\n",
      "\n",
      "log_data with column page == \"NextSong\"\n",
      "6820\n",
      "+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+----+------+---+---------+------+\n",
      "|artist|auth|firstName|gender|itemInSession|lastName|length|level|location|method|page|registration|sessionId|song|status| ts|userAgent|userId|\n",
      "+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+----+------+---+---------+------+\n",
      "+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+----+------+---+---------+------+\n",
      "\n",
      "users_table\n",
      "6820\n",
      "+-------+----------+---------+------+-----+\n",
      "|user_id|first_name|last_name|gender|level|\n",
      "+-------+----------+---------+------+-----+\n",
      "+-------+----------+---------+------+-----+\n",
      "\n",
      "time_table\n",
      "6820\n",
      "+-------------------+----+---+----+-----+----+\n",
      "|         start_time|hour|day|week|month|year|\n",
      "+-------------------+----+---+----+-----+----+\n",
      "|2018-11-15 00:00:26|   0| 15|  46|   11|2018|\n",
      "|2018-11-15 00:00:21|   0| 15|  46|   11|2018|\n",
      "|2018-11-15 00:00:41|   0| 15|  46|   11|2018|\n",
      "|2018-11-15 03:00:09|   3| 15|  46|   11|2018|\n",
      "|2018-11-15 05:00:55|   5| 15|  46|   11|2018|\n",
      "+-------------------+----+---+----+-----+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "songplays_table\n",
      "21\n",
      "+-----------+----------+-------+-----+-------+---------+----------+--------+---------+-----+----+\n",
      "|songplay_id|start_time|user_id|level|song_id|artist_id|session_id|location|userAgent|month|year|\n",
      "+-----------+----------+-------+-----+-------+---------+----------+--------+---------+-----+----+\n",
      "+-----------+----------+-------+-----+-------+---------+----------+--------+---------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def process_log_data(spark, input_data, output_data):\n",
    "    # get filepath to log data file\n",
    "    log_data = input_data + \"log_data/*/*/*.json\"\n",
    "    log_data1 = input_data + \"log_data/*/*/2018-11-12-events.json\"\n",
    "\n",
    "    # define schema\n",
    "    logdata_schema = StructType([\n",
    "        StructField(\"artist\", StringType(), True),\n",
    "        StructField(\"auth\", StringType(), True),\n",
    "        StructField(\"firstName\", StringType(), True),\n",
    "        StructField(\"gender\", StringType(), True),\n",
    "        StructField(\"itemInSession\", LongType(), True),\n",
    "        StructField(\"lastName\", StringType(), True),\n",
    "        StructField(\"length\", DoubleType(), True),\n",
    "        StructField(\"level\", StringType(), True),\n",
    "        StructField(\"location\", StringType(), True),\n",
    "        StructField(\"method\", StringType(), True),\n",
    "        StructField(\"page\", StringType(), True),\n",
    "        StructField(\"registration\", DoubleType(), True),\n",
    "        StructField(\"sessionId\", LongType(), True),\n",
    "        StructField(\"song\", StringType(), True),\n",
    "        StructField(\"status\", LongType(), True),\n",
    "        StructField(\"ts\", LongType(), True),\n",
    "        StructField(\"userAgent\", StringType(), True),\n",
    "        StructField(\"userId\", IntegerType(), True),\n",
    "    ])\n",
    "    \n",
    "    # read log data file\n",
    "    df = spark.read.json(log_data, schema=logdata_schema)\n",
    "    df.printSchema()\n",
    "    print('log_data')\n",
    "    print(df.count())\n",
    "    df.show(10)\n",
    "    \n",
    "    # filter by actions for song plays\n",
    "    df = df.filter(col(\"page\") == \"NextSong\")\n",
    "    print('log_data with column page == \"NextSong\"')\n",
    "    print(df.count())\n",
    "    df.show(5)\n",
    "\n",
    "    \n",
    "    # extract columns for users table    \n",
    "    users_table = df.select([col(\"userId\").alias(\"user_id\"),\n",
    "                             col(\"firstName\").alias(\"first_name\"),\n",
    "                             col(\"lastName\").alias(\"last_name\"),\n",
    "                             col(\"gender\"),\n",
    "                             col(\"level\")])\n",
    "    print('users_table')\n",
    "    print(users_table.count())\n",
    "    users_table.show(5)\n",
    "\n",
    "    # write users table to parquet files\n",
    "    users_table_path = os.path.join(output_data, \"users_table.parquet\")\n",
    "    if not os.path.exists(users_table_path):\n",
    "        users_table.write.parquet(users_table_path)\n",
    "    \n",
    "    tsFormat = \"yyyy-MM-dd HH:MM:ss z\"\n",
    "    # Converting ts to a timestamp format\n",
    "    time_table = df.withColumn('ts',\n",
    "                               to_timestamp(\n",
    "                                   date_format(\n",
    "                                       (df.ts /1000).cast(dataType=TimestampType()), \n",
    "                                       tsFormat), \n",
    "                                   tsFormat)\n",
    "                              )\n",
    "    \n",
    "    time_table = time_table.select(col(\"ts\").alias(\"start_time\"),\n",
    "                                   hour(col(\"ts\")).alias(\"hour\"),\n",
    "                                   dayofmonth(col(\"ts\")).alias(\"day\"), \n",
    "                                   weekofyear(col(\"ts\")).alias(\"week\"), \n",
    "                                   month(col(\"ts\")).alias(\"month\"),\n",
    "                                   year(col(\"ts\")).alias(\"year\"))\n",
    "    print('time_table')\n",
    "    print(time_table.count())\n",
    "    time_table.show(5)\n",
    "    \n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    if not os.path.exists(os.path.join(output_data, 'time')):\n",
    "        time_table.write.partitionBy(\"year\",\"month\").parquet(os.path.join(output_data, 'time'))\n",
    "    \n",
    "    # read in song data to use for songplays table\n",
    "    song_data = input_data + \"song_data/*/*/*/*.json\"\n",
    "    song_df = spark.read.json(song_data)\n",
    "\n",
    "    # extract columns from joined song and log datasets to create songplays table \n",
    "    songplays_df = song_df.join(df, song_df.artist_name == df.artist) \\\n",
    "                          .withColumn(\"songplay_id\", monotonically_increasing_id()) \\\n",
    "                          .withColumn('start_time', to_timestamp(date_format(\n",
    "                                          (col(\"ts\") /1000)\n",
    "                                           .cast(dataType=TimestampType()), tsFormat),tsFormat))\n",
    "    \n",
    "    songplays_table = songplays_df.select(\"songplay_id\",\n",
    "                                          \"start_time\",\n",
    "                                          col(\"userId\").alias(\"user_id\"),\n",
    "                                          \"level\",\n",
    "                                          \"song_id\",\n",
    "                                          \"artist_id\",\n",
    "                                          col(\"sessionId\").alias(\"session_id\"),\n",
    "                                          col(\"artist_location\").alias(\"location\"),\n",
    "                                          \"userAgent\",\n",
    "                                          month(col(\"start_time\")).alias(\"month\"),\n",
    "                                          year(col(\"start_time\")).alias(\"year\"))\n",
    "    print('songplays_table')\n",
    "    print(songplays_table.count())\n",
    "    songplays_table.show(5)\n",
    "           \n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    if not os.path.exists(os.path.join(output_data, 'songplays_table')):\n",
    "        time_table.write.partitionBy(\"year\",\"month\").parquet(os.path.join(output_data, 'songplays_table'))\n",
    "\n",
    "process_log_data(spark, input_data, output_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
